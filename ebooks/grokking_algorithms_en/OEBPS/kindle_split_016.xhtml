<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
<link href="Styles/Style00.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style01.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style02.css" rel="stylesheet" type="text/css" />
<link href="Styles/Style03.css" rel="stylesheet" type="text/css" />
<style type="text/css" title="ibis-book">
    #sbo-rt-content .calibre{display:block;font-size:1em;margin-bottom:0;margin-left:5pt;margin-right:5pt;margin-top:0;padding-left:0;padding-right:0}#sbo-rt-content .calibre1{display:block;font-size:2em;font-weight:bold;line-height:1.2;margin-bottom:.67em;margin-left:0;margin-right:0;margin-top:.67em;text-align:center}#sbo-rt-content .calibre10{display:table-row-group;vertical-align:middle}#sbo-rt-content .calibre11{display:table-row;vertical-align:inherit}#sbo-rt-content .calibre12{display:block;font-family:"Liberation Mono",monospace;font-size:.77778em;margin-bottom:1em;margin-left:5px;margin-right:0;margin-top:1em;white-space:pre-wrap}#sbo-rt-content .calibre13{display:block;font-size:1.125em;font-weight:bold;line-height:1.2;margin-bottom:0;margin-left:0;margin-right:0;margin-top:40px}#sbo-rt-content .calibre14{display:block}#sbo-rt-content .calibre15{display:block;margin-bottom:1em;margin-left:1em;margin-right:1em;margin-top:1em}#sbo-rt-content .calibre16{display:block}#sbo-rt-content .calibre17{display:block;margin-bottom:1em;margin-left:0;margin-right:0;margin-top:1em}#sbo-rt-content .calibre18{display:block;list-style-type:disc;margin-bottom:1em;margin-right:0;margin-top:1em}#sbo-rt-content .calibre19{display:list-item}#sbo-rt-content .calibre2{height:auto;width:auto}#sbo-rt-content .calibre20{font-weight:bold}#sbo-rt-content .calibre21{font-family:"Liberation Mono",monospace}#sbo-rt-content .calibre22{display:block;font-weight:bold;margin-bottom:0;margin-left:0;margin-right:0;margin-top:30px}#sbo-rt-content .calibre23{display:table-column-group}#sbo-rt-content .calibre24{display:table-cell;padding-bottom:1px;padding-left:1px;padding-right:1px;padding-top:1px;text-align:inherit;vertical-align:inherit}#sbo-rt-content .calibre25{font-size:.75em;line-height:normal;vertical-align:sub}#sbo-rt-content .calibre26{font-size:.75em;line-height:normal;vertical-align:super}#sbo-rt-content .calibre27{display:block;list-style-type:decimal;margin-bottom:1em;margin-right:0;margin-top:1em}#sbo-rt-content .calibre28{font-size:smaller;line-height:normal;vertical-align:super}#sbo-rt-content .calibre29{font-size:.75em}#sbo-rt-content .calibre3{font-family:"Times";line-height:1.2}#sbo-rt-content .calibre30{font-size:.77778em;line-height:normal;vertical-align:super}#sbo-rt-content .calibre31{font-size:.71429em}#sbo-rt-content .calibre4{font-family:"Times"}#sbo-rt-content .calibre5{display:block;font-family:"Liberation Mono",monospace;font-size:.75em;margin-bottom:1em;margin-left:5px;margin-right:0;margin-top:1em;white-space:pre-wrap}#sbo-rt-content .calibre6{font-style:italic}#sbo-rt-content .calibre7{border-collapse:separate;border-spacing:2px;display:table;margin-bottom:0;margin-top:0;text-indent:0}#sbo-rt-content .calibre8{display:table-column-group;text-align:left}#sbo-rt-content .calibre9{display:table-column}#sbo-rt-content .center{display:block;font-weight:bold;margin-bottom:0;margin-left:0;margin-right:0;margin-top:30px;text-align:center}#sbo-rt-content .center1{display:block;margin-bottom:1em;margin-left:0;margin-right:0;margin-top:1em;text-align:center}#sbo-rt-content .docTableCell{display:table-cell;font-size:.75em;padding-bottom:1px;padding-left:1px;padding-right:1px;padding-top:1px;text-align:left;vertical-align:inherit}#sbo-rt-content .ind{display:block;margin-bottom:8px;margin-right:0;margin-top:0;text-align:left;text-indent:0}#sbo-rt-content .noind{display:block;margin-bottom:1em;margin-left:0;margin-right:0;margin-top:25px;text-indent:0}#sbo-rt-content .noindclose{display:block;margin-bottom:1em;margin-left:0;margin-right:0;margin-top:1em;text-indent:0}#sbo-rt-content .notetitle{display:block;font-size:.75em;font-weight:bold;margin-bottom:0;margin-left:0;margin-right:0;margin-top:2px}#sbo-rt-content .part{display:block;font-size:1.41667em;font-weight:bold;line-height:1.2;margin-bottom:0;margin-left:0;margin-right:0;margin-top:.83em}#sbo-rt-content .smaller{display:block;font-size:.75em;margin-bottom:1em;margin-left:1em;margin-right:1em;margin-top:1em}#sbo-rt-content .toc{display:block;margin-bottom:1em;margin-left:15px;margin-right:1em;margin-top:15px;text-indent:0}
    </style>
<style type="text/css" title="ibis-book">
    @page{margin-bottom:5pt;margin-top:5pt}
    </style>
<style type="text/css" id="font-styles">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: &lt;%= font_size %&gt; !important; }</style>
<style type="text/css" id="font-family">#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: &lt;%= font_family %&gt; !important; }</style>
<style type="text/css" id="column-width">#sbo-rt-content { max-width: &lt;%= column_width %&gt;% !important; margin: 0 auto !important; }</style>

<style type="text/css">body{margin:1em;}#sbo-rt-content *{text-indent:0pt!important;}#sbo-rt-content .bq{margin-right:1em!important;}body{background-color:transparent!important;}#sbo-rt-content *{word-wrap:break-word!important;word-break:break-word!important;}#sbo-rt-content table,#sbo-rt-content pre{overflow-x:unset!important;overflow:unset!important;overflow-y:unset!important;white-space:pre-wrap!important;}</style></head>
<body><div id="sbo-rt-content" class="calibre"><h2 id="ch10" class="part">Chapter 10. <a id="ch10__title" class="calibre3"/>K-nearest neighbors
      </h2>
      
      
      
      <p class="center1"><img src="Images/common.jpg" alt="" class="calibre2" width="134" height="148"/></p>
      
      
      <p class="noind"><a id="iddle1042" class="calibre4"/><a id="iddle1236" class="calibre4"/><b class="calibre20">In this chapter</b></p>
      
      <p class="calibre17"/>
      <ul class="calibre18">
         
         <li class="calibre19">You learn to build a classification system using the k-nearest neighbors algorithm.
            
         </li>
         
         <li class="calibre19">You learn about feature extraction.
            
         </li>
         
         <li class="calibre19">You learn about regression: predicting a number, like the value of a stock tomorrow, or how much a user will enjoy a movie.
            
         </li>
         
         <li class="calibre19">You learn about the use cases and limitations of k-nearest neighbors.
            
         </li>
         
      </ul>
      
      
      <h3 id="ch10lev1sec1" class="calibre13"><a id="ch10lev1sec1__title" class="calibre3"/>Classifying oranges vs. grapefruit
      </h3>
      
      <p class="noind">Look at this fruit. Is it an orange or a grapefruit? Well, I know that grapefruits are generally bigger and redder.</p>
      
      
      
      <p class="center1"><img src="Images/187fig01.jpg" alt="" class="calibre2" width="254" height="131"/></p>
      
      
      <p class="noind">My thought process is something like this: I have a graph in my mind.</p>
      
      
      
      <p class="center1"><img src="Images/188fig01.jpg" alt="" class="calibre2" width="357" height="389"/></p>
      
      
      <p class="noind">Generally speaking, the bigger, redder fruit are grapefruits. This fruit is big and red, so it’s probably a grapefruit. But
         what if you get a fruit like this?
      </p>
      
      
      
      <p class="center1"><img src="Images/188fig02.jpg" alt="" class="calibre2" width="356" height="335"/></p>
      
      
      <p class="noind">How would you <i class="calibre6">classify</i> this fruit? One way is to look at the neighbors of this spot. Take a look at the three closest neighbors of this spot.
      </p>
      
      
      
      
      <p class="center1"><img src="Images/189fig01.jpg" alt="" class="calibre2" width="359" height="303"/></p>
      
      
      <p class="noind"><a id="iddle1041" class="calibre4"/><a id="iddle1109" class="calibre4"/><a id="iddle1235" class="calibre4"/><a id="iddle1297" class="calibre4"/>More neighbors are oranges than grapefruit. So this fruit is probably an orange. Congratulations: You just used the <i class="calibre6">k-nearest neighbors</i> (KNN) algorithm for <i class="calibre6">classification</i>! The whole algorithm is pretty simple.
      </p>
      
      <p class="center1"><img src="Images/189fig02_alt.jpg" alt="" class="calibre2" width="590" height="195"/></p>
      
      <p class="noind">The KNN algorithm is simple but useful! If you’re trying to classify something, you might want to try KNN first. Let’s look
         at a more real-world example.
      </p>
      
      
      
      <h3 id="ch10lev1sec2" class="calibre13"><a id="ch10lev1sec2__title" class="calibre3"/>Building a recommendations system
      </h3>
      
      <p class="noind">Suppose you’re Netflix, and you want to build a movie recommendations system for your users. On a high level, this is similar
         to the grapefruit problem!
      </p>
      
      <p class="noind">You can plot every user on a graph.</p>
      
      
      
      <p class="center1"><img src="Images/190fig01.jpg" alt="" class="calibre2" width="331" height="310"/></p>
      
      
      <p class="noind">These users are plotted by similarity, so users with similar taste are plotted closer together. Suppose you want to recommend
         movies for Priyanka. Find the five users closest to her.
      </p>
      
      
      
      <p class="center1"><img src="Images/190fig02.jpg" alt="" class="calibre2" width="352" height="315"/></p>
      
      
      <p class="noind">Justin, JC, Joey, Lance, and Chris all have similar taste in movies. So whatever movies <i class="calibre6">they</i> like, Priyanka will probably like too!
      </p>
      
      <p class="noind">Once you have this graph, building a recommendations system is easy. If Justin likes a movie, recommend it to Priyanka.</p>
      
      <p class="center1"><img src="Images/190fig03_alt.jpg" alt="" class="calibre2" width="590" height="168"/></p>
      
      <p class="noind"><a id="iddle1290" class="calibre4"/>But there’s still a big piece missing. You graphed the users by similarity. How do you figure out how similar two users are?
      </p>
      
      
      <h4 id="ch10lev2sec1" class="calibre22"><a id="ch10lev2sec1__title" class="calibre4"/>Feature extraction
      </h4>
      
      <p class="noind">In the grapefruit example, you compared fruit based on how big they are and how red they are. Size and color are the <i class="calibre6">features</i> you’re comparing. Now suppose you have three fruit. You can extract the features.
      </p>
      
      
      
      <p class="center1"><img src="Images/191fig01.jpg" alt="" class="calibre2" width="466" height="228"/></p>
      
      
      <p class="noind">Then you can graph the three fruit.</p>
      
      
      
      <p class="center1"><img src="Images/191fig02.jpg" alt="" class="calibre2" width="341" height="306"/></p>
      
      
      <p class="noind">From the graph, you can tell visually that fruits A and B are similar. Let’s measure how close they are. To find the distance
         between two points, you use the Pythagorean formula.
      </p>
      
      
      
      <p class="center1"><img src="Images/191fig03.jpg" alt="" class="calibre2" width="338" height="86"/></p>
      
      
      <p class="noind">Here’s the distance between A and B, for example.</p>
      
      
      
      <p class="center1"><img src="Images/192fig01.jpg" alt="" class="calibre2" width="239" height="241"/></p>
      
      
      <p class="noind">The distance between A and B is 1. You can find the rest of the distances, too.</p>
      
      
      
      <p class="center1"><img src="Images/192fig02.jpg" alt="" class="calibre2" width="294" height="268"/></p>
      
      
      <p class="noind">The distance formula confirms what you saw visually: fruits A and B are similar.</p>
      
      <p class="noind">Suppose you’re comparing Netflix users, instead. You need some way to graph the users. So, you need to convert each user to
         a set of coordinates, just as you did for fruit.
      </p>
      
      
      
      <p class="center1"><img src="Images/192fig03.jpg" alt="" class="calibre2" width="309" height="246"/></p>
      
      
      <p class="noind">Once you can graph users, you can measure the distance between them.</p>
      
      <p class="noind">Here’s how you can convert users into a set of numbers. When users sign up for Netflix, have them rate some categories of
         movies based on how much they like those categories. For each user, you now have a set of ratings!
      </p>
      
      <p class="center1"><img src="Images/193fig01_alt.jpg" alt="" class="calibre2" width="590" height="297"/></p>
      
      <p class="noind">Priyanka and Justin like Romance and hate Horror. Morpheus likes Action but hates Romance (he hates when a good action movie
         gets ruined by a cheesy romantic scene). Remember how in oranges versus grapefruit, each fruit was represented by a set of
         two numbers? Here, each user is represented by a set of five numbers.
      </p>
      
      
      
      <p class="center1"><img src="Images/193fig02.jpg" alt="" class="calibre2" width="390" height="189"/></p>
      
      
      <p class="noind">A mathematician would say, instead of calculating the distance in two dimensions, you’re now calculating the distance in <i class="calibre6">five</i> dimensions. But the distance formula remains the same.
      </p>
      
      
      
      
      <p class="center1"><img src="Images/194fig01.jpg" alt="" class="calibre2" width="500" height="65"/></p>
      
      
      <p class="noind"><a id="iddle1132" class="calibre4"/>It just involves a set of five numbers instead of a set of two numbers.
      </p>
      
      <p class="noind">The distance formula is flexible: you could have a set of a <i class="calibre6">million</i> numbers and still use the same old distance formula to find the distance. Maybe you’re wondering, “What does <i class="calibre6">distance</i> mean when you have five numbers?” The distance tells you how similar those sets of numbers are.
      </p>
      
      
      
      <p class="center1"><img src="Images/194fig02.jpg" alt="" class="calibre2" width="500" height="212"/></p>
      
      
      <p class="noind">Here’s the distance between Priyanka and Justin.</p>
      
      <p class="noind">Priyanka and Justin are pretty similar. What’s the difference between Priyanka and Morpheus? Calculate the distance before
         moving on.
      </p>
      
      <p class="noind">Did you get it right? Priyanka and Morpheus are 24 apart. The distance tells you that Priyanka’s tastes are more like Justin’s
         than Morpheus’s.
      </p>
      
      <p class="noind">Great! Now recommending movies to Priyanka is easy: if Justin likes a movie, recommend it to Priyanka, and vice versa. You
         just built a movie recommendations system!
      </p>
      
      <p class="noind">If you’re a Netflix user, Netflix will keep telling you, “Please rate more movies. The more movies you rate, the better your
         recommendations will be.” Now you know why. The more movies you rate, the more accurately Netflix can see what other users
         you’re similar to.
      </p>
      
      
      
      
      
      <h3 id="ch10lev1sec3" class="calibre13"><a id="ch10lev1sec3__title" class="calibre3"/>Exercises
      </h3>
      
      <p class="calibre17"><a id="ch10qa1" class="calibre4"/></p>
      <blockquote class="calibre15">
         <p class="calibre17"><a id="ch10qa1qe1" class="calibre4"/></p>
         <p class="calibre17"><a id="ch10qa1q1" class="calibre4"/><b class="calibre20">10.1 </b></p><p class="noind"><a id="iddle1043" class="calibre4"/><a id="iddle1237" class="calibre4"/>In the Netflix example, you calculated the distance between two different users using the distance formula. But not all users
               rate movies the same way. Suppose you have two users, Yogi and Pinky, who have the same taste in movies. But Yogi rates any
               movie he likes as a 5, whereas Pinky is choosier and reserves the 5s for only the best. They’re well matched, but according
               to the distance algorithm, they aren’t neighbors. How would you take their different rating strategies into account?
            </p>
         <p class="calibre17"/>
      </blockquote>
      <blockquote class="calibre15">
         <p class="calibre17"><a id="ch10qa1qe2" class="calibre4"/></p>
         <p class="calibre17"><a id="ch10qa1q2" class="calibre4"/><b class="calibre20">10.2 </b></p><p class="noind">Suppose Netflix nominates a group of “influencers.” For example, Quentin Tarantino and Wes Anderson are influencers on Netflix,
               so their ratings count for more than a normal user’s. How would you change the recommendations system so it’s biased toward
               the ratings of influencers?
            </p>
         <p class="calibre17"/>
      </blockquote>
      
      
      <h4 id="ch10lev2sec2" class="calibre22"><a id="ch10lev2sec2__title" class="calibre4"/>Regression
      </h4>
      
      <p class="noind">Suppose you want to do more than just recommend a movie: you want to guess how Priyanka will rate this movie. Take the five
         people closest to her.
      </p>
      
      <p class="noind">By the way, I keep talking about the closest five people. There’s nothing</p>
      
      
      
      <p class="center1"><img src="Images/195fig01.jpg" alt="" class="calibre2" width="361" height="315"/></p>
      
      
      <p class="noind">special about the number 5: you could do the closest 2, or 10, or 10,000. That’s why the algorithm is called k-nearest neighbors
         and not five-nearest neighbors!
      </p>
      
      <p class="noind">Suppose you’re trying to guess a rating for <i class="calibre6">Pitch Perfect</i>. Well, how did Justin, JC, Joey, Lance, and Chris rate it?
      </p>
      
      
      
      
      <p class="center1"><img src="Images/196fig01.jpg" alt="" class="calibre2" width="163" height="182"/></p>
      
      
      <p class="noind"><a id="iddle1302" class="calibre4"/>You could take the average of their ratings and get 4.2 stars. That’s called <i class="calibre6">regression</i>. These are the two basic things you’ll do with KNN—classification and regression:
      </p>
      
      <p class="calibre17"/>
      <ul class="calibre18">
         
         <li class="calibre19">Classification = categorization into a group
            
         </li>
         
         <li class="calibre19">Regression = predicting a response (like a number)
            
         </li>
         
      </ul>
      
      <p class="noind">Regression is very useful. Suppose you run a small bakery in Berkeley, and you make fresh bread every day. You’re trying to
         predict how many loaves to make for today. You have a set of features:
      </p>
      
      <p class="calibre17"/>
      <ul class="calibre18">
         
         <li class="calibre19">Weather on a scale of 1 to 5 (1 = bad, 5 = great).
            
         </li>
         
         <li class="calibre19">Weekend or holiday? (1 if it’s a weekend or a holiday, 0 otherwise.)
            
         </li>
         
         <li class="calibre19">Is there a game on? (1 if yes, 0 if no.)
            
         </li>
         
      </ul>
      
      
      
      <p class="center1"><img src="Images/196fig02.jpg" alt="" class="calibre2" width="215" height="135"/></p>
      
      
      <p class="noind">And you know how many loaves of bread you’ve sold in the past for different sets of features.</p>
      
      
      
      <p class="center1"><img src="Images/196fig03.jpg" alt="" class="calibre2" width="500" height="225"/></p>
      
      
      <p class="noind">Today is a weekend day with good weather. Based on the data you just saw, how many loaves will you sell? Let’s use KNN, where
         K = 4. First, figure out the four nearest neighbors for this point.
      </p>
      
      
      
      <p class="center1"><img src="Images/197fig01.jpg" alt="" class="calibre2" width="189" height="51"/></p>
      
      
      <p class="noind">Here are the distances. A, B, D, and E are the closest.</p>
      
      
      
      <p class="center1"><img src="Images/197fig02.jpg" alt="" class="calibre2" width="131" height="261"/></p>
      
      
      <p class="noind">Take an average of the loaves sold on those days, and you get 218.75. That’s how many loaves you should make for today!</p>
      
      <table width="100%" border="1" cellspacing="5" class="calibre7">
         <colgroup class="calibre23">
            <col width="550" class="calibre9"/>
         </colgroup>
         <tbody class="calibre10">
            <tr class="calibre11">
               <td class="calibre24"/>
            </tr>
         </tbody>
      </table>
      <div class="calibre14">
         
         <b id="ch10sb01" class="calibre20">Cosine similarity</b>
         
         <p class="noind">So far, you’ve been using the distance formula to compare the distance between two users. Is this the best formula to use?
            A common one used in practice is <i class="calibre6">cosine similarity</i>. Suppose two users are similar, but one of them is more conservative in their ratings. They both loved Manmohan Desai’s <i class="calibre6">Amar Akbar Anthony</i>. Paul rated it 5 stars, but Rowan rated it 4 stars. If you keep using the distance formula, these two users might not be
            each other’s neighbors, even though they have similar taste.
         </p>
         
         <p class="noind">Cosine similarity doesn’t measure the distance between two vectors. Instead, it compares the angles of the two vectors. It’s
            better at dealing with cases like this. Cosine similarity is out of the scope of this book, but look it up if you use KNN!
         </p>
         
      </div>
      <table width="100%" border="1" cellspacing="5" class="calibre7">
         <colgroup class="calibre23">
            <col width="550" class="calibre9"/>
         </colgroup>
         <tbody class="calibre10">
            <tr class="calibre11">
               <td class="calibre24"/>
            </tr>
         </tbody>
      </table>
      
      
      
      
      <h4 id="ch10lev2sec3" class="calibre22"><a id="ch10lev2sec3__title" class="calibre4"/>Picking good features
      </h4>
      
      
      
      <p class="center1"><img src="Images/198fig01.jpg" alt="" class="calibre2" width="266" height="97"/></p>
      
      
      <p class="noind">To figure out recommendations, you had users rate categories of movies. What if you had them rate pictures of cats instead?
         Then you’d find users who rated those pictures similarly. This would probably be a worse recommendations engine, because the
         “features” don’t have a lot to do with taste in movies!
      </p>
      
      <p class="noind">Or suppose you ask users to rate movies so you can give them recommendations—but you only ask them to rate <i class="calibre6">Toy Story</i>, <i class="calibre6">Toy Story 2</i>, and <i class="calibre6">Toy Story 3</i>. This won’t tell you a lot about the users’ movie tastes!
      </p>
      
      <p class="noind">When you’re working with KNN, it’s really important to pick the right features to compare against. Picking the right features
         means
      </p>
      
      <p class="calibre17"/>
      <ul class="calibre18">
         
         <li class="calibre19">Features that directly correlate to the movies you’re trying to recommend
            
         </li>
         
         <li class="calibre19">Features that don’t have a bias (for example, if you ask the users to only rate comedy movies, that doesn’t tell you whether
            they like action movies)
            
         </li>
         
      </ul>
      
      <p class="noind">Do you think ratings are a good way to recommend movies? Maybe I rated <i class="calibre6">The Wire</i> more highly than <i class="calibre6">House Hunters</i>, but I actually spend more time watching <i class="calibre6">House Hunters</i>. How would you improve this Netflix recommendations system?
      </p>
      
      <p class="noind">Going back to the bakery: can you think of two good and two bad features you could have picked for the bakery? Maybe you need
         to make more loaves after you advertise in the paper. Or maybe you need to make more loaves on Mondays.
      </p>
      
      <p class="noind">There’s no one right answer when it comes to picking good features. You have to think about all the different things you need
         to consider.
      </p>
      
      
      
      
      <h3 id="ch10lev1sec4" class="calibre13"><a id="ch10lev1sec4__title" class="calibre3"/>Exercise
      </h3>
      
      <p class="calibre17"><a id="ch10qa2" class="calibre4"/></p>
      <blockquote class="calibre15">
         <p class="calibre17"><a id="ch10qa2qe1" class="calibre4"/></p>
         <p class="calibre17"><a id="ch10qa2q1" class="calibre4"/><b class="calibre20">10.3 </b></p><p class="noind">Netflix has millions of users. The earlier example looked at the five closest neighbors for building the recommendations system.
               Is this too low? Too high?
            </p>
         <p class="calibre17"/>
      </blockquote>
      
      
      
      
      <h3 id="ch10lev1sec5" class="calibre13"><a id="ch10lev1sec5__title" class="calibre3"/>Introduction to machine learning
      </h3>
      
      
      
      <p class="center1"><img src="Images/199fig01.jpg" alt="" class="calibre2" width="260" height="270"/></p>
      
      
      <p class="noind"><a id="iddle1044" class="calibre4"/><a id="iddle1238" class="calibre4"/><a id="iddle1260" class="calibre4"/><a id="iddle1274" class="calibre4"/>KNN is a really useful algorithm, and it’s your introduction to the magical world of machine learning! Machine learning is
         all about making your computer more intelligent. You already saw one example of machine learning: building a recommendations
         system. Let’s look at some other examples.
      </p>
      
      
      <h4 id="ch10lev2sec4" class="calibre22"><a id="ch10lev2sec4__title" class="calibre4"/>OCR
      </h4>
      
      <p class="noind">OCR stands for <i class="calibre6">optical character recognition</i>. It means you can take a photo of a page of text, and your computer will automatically read the text for you. Google uses
         OCR to digitize books. How does OCR work? For example, consider this number.
      </p>
      
      
      
      <p class="center1"><img src="Images/199fig02.jpg" alt="" class="calibre2" width="88" height="103"/></p>
      
      
      <p class="noind">How would you automatically figure out what number this is? You can use KNN for this:</p>
      
      <p class="calibre17"/>
      <ol class="calibre27">
         
         <li class="calibre19">Go through a lot of images of numbers, and extract features of those numbers.
            
         </li>
         
         <li class="calibre19">When you get a new image, extract the features of that image, and see what its nearest neighbors are!
            
         </li>
         
      </ol>
      
      <p class="noind">It’s the same problem as oranges versus grapefruit. Generally speaking, OCR algorithms measure lines, points, and curves.</p>
      
      
      
      <p class="center1"><img src="Images/199fig03.jpg" alt="" class="calibre2" width="464" height="167"/></p>
      
      
      <p class="noind">Then, when you get a new character, you can extract the same features from it.</p>
      
      <p class="noind"><a id="iddle1266" class="calibre4"/><a id="iddle1340" class="calibre4"/><a id="iddle1357" class="calibre4"/>Feature extraction is a lot more complicated in OCR than the fruit example. But it’s important to understand that even complex
         technologies build on simple ideas, like KNN. You could use the same ideas for speech recognition or face recognition. When
         you upload a photo to Facebook, sometimes it’s smart enough to tag people in the photo automatically. That’s machine learning
         in action!
      </p>
      
      <p class="noind">The first step of OCR, where you go through images of numbers and extract features, is called <i class="calibre6">training</i>. Most machine-learning algorithms have a training step: before your computer can do the task, it must be trained. The next
         example involves spam filters, and it has a training step.
      </p>
      
      
      
      <h4 id="ch10lev2sec5" class="calibre22"><a id="ch10lev2sec5__title" class="calibre4"/>Building a spam filter
      </h4>
      
      <p class="noind">Spam filters use another simple algorithm called the <i class="calibre6">Naive Bayes classifier</i>. First, you train your Naive Bayes classifier on some data.
      </p>
      
      <p class="center1"><img src="Images/200fig01_alt.jpg" alt="" class="calibre2" width="590" height="262"/></p>
      
      <p class="noind">Suppose you get an email with the subject “collect your million dollars now!” Is it spam? You can break this sentence into
         words. Then, for each word, see what the probability is for that word to show up in a spam email. For example, in this very
         simple model, the word <i class="calibre6">million</i> only appears in spam emails. Naive Bayes figures out the probability that something is likely to be spam. It has applications
         similar to KNN.
      </p>
      
      <p class="noind"><a id="iddle1351" class="calibre4"/>For example, you could use Naive Bayes to categorize fruit: you have a fruit that’s big and red. What’s the probability that
         it’s a grapefruit? It’s another simple algorithm that’s fairly effective. We love those algorithms!
      </p>
      
      
      
      <p class="center1"><img src="Images/201fig01.jpg" alt="" class="calibre2" width="248" height="274"/></p>
      
      
      
      
      <h4 id="ch10lev2sec6" class="calibre22"><a id="ch10lev2sec6__title" class="calibre4"/>Predicting the stock market
      </h4>
      
      <p class="noind">Here’s something that’s hard to do with machine learning: really predicting whether the stock market will go up or down. How
         do you pick good features in a stock market? Suppose you say that if the stock went up yesterday, it will go up today. Is
         that a good feature? Or suppose you say that the stock will always go down in May. Will that work? There’s no guaranteed way
         to use past numbers to predict future performance. Predicting the future is hard, and it’s almost impossible when there are
         so many variables involved.
      </p>
      
      
      
      
      <h3 id="ch10lev1sec6" class="calibre13"><a id="ch10lev1sec6__title" class="calibre3"/>Recap
      </h3>
      
      <p class="noind">I hope this gives you an idea of all the different things you can do with KNN and with machine learning! Machine learning
         is an interesting field that you can go pretty deep into if you decide to:
      </p>
      
      <p class="calibre17"/>
      <ul class="calibre18">
         
         <li class="calibre19">KNN is used for classification and regression and involves looking at the k-nearest neighbors.
            
         </li>
         
         <li class="calibre19">Classification = categorization into a group.
            
         </li>
         
         <li class="calibre19">Regression = predicting a response (like a number).
            
         </li>
         
         <li class="calibre19">Feature extraction means converting an item (like a fruit or a user) into a list of numbers that can be compared.
            
         </li>
         
         <li class="calibre19">Picking good features is an important part of a successful KNN algorithm.
            
         </li>
         
      </ul>
      
      
      
      
      <div class="calibre16" id="calibre_pb_28"/>
</div>



  </body>
</html>